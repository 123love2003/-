{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c106c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机模块\n",
    "import random\n",
    "\n",
    "# 绘图模块\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 自定义模块\n",
    "from torchLearning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5b8824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    数据切分函数\n",
    "    \n",
    "    :param batch_size: 每个子数据集包含多少数据\n",
    "    :param featurs: 输入的特征张量\n",
    "    :param labels：输入的标签张量\n",
    "    :return l：包含batch_size个列表，每个列表切分后的特征和标签所组成 \n",
    "    \"\"\"\n",
    "    # 获取总样本数量\n",
    "    num_examples = len(features)\n",
    "    \n",
    "    # 创建从0到num_examples-1的索引列表\n",
    "    indices = list(range(num_examples))\n",
    "    \n",
    "    # 随机打乱索引顺序，实现数据的随机化\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # 初始化空列表用于存储所有批量数据\n",
    "    l = []\n",
    "    \n",
    "    # 以batch_size为步长遍历所有样本索引\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # i 是当前批量在总数据中的起始位置\n",
    "        # 获取当前批量的索引范围，处理最后一个批量可能不足的情况\n",
    "        j = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        \n",
    "        # 使用索引选择对应的特征和标签数据，并作为一个批量添加到结果列表\n",
    "        l.append([torch.index_select(features, 0, j), torch.index_select(labels, 0, j)])\n",
    "    \n",
    "    # 返回包含所有批量的列表\n",
    "    return l\n",
    "def tensorGenReg(num_examples=1000, w=[2, -1, 1], bias=True, delta=0.01, deg=1):\n",
    "    \"\"\"\n",
    "    生成带多项式特征的人工合成数据集（可自定义噪声和多项式阶数）\n",
    "    \n",
    "    参数：\n",
    "        num_examples (int): 样本数量（默认1000）\n",
    "        w (list/tensor): 真实权重系数（例如 [2,-1,1] 表示 2*x1 -1*x2 +1*x1^2）\n",
    "        bias (bool): 是否包含偏置项（默认True）\n",
    "        delta (float): 噪声标准差（默认0.01）\n",
    "        deg (int): 多项式阶数（默认1，即线性）\n",
    "        \n",
    "    返回：\n",
    "        features (Tensor): 生成的特征矩阵 (num_examples, num_features)\n",
    "        labels (Tensor): 生成的标签 (num_examples, 1)\n",
    "    \"\"\"\n",
    "    # 当包含偏置项时\n",
    "    if bias==True:\n",
    "        # 输入特征数量 = 权重数量-1（最后一个权重是偏置项）\n",
    "        num_inputs=len(w)-1\n",
    "        # 生成随机特征矩阵（标准正态分布）\n",
    "        features_ture=torch.randn(num_examples,num_inputs)\n",
    "        # 提取权重（排除最后一个偏置项）并转为列向量\n",
    "        w_true=torch.tensor(w[:-1]).reshape(-1,1).float()\n",
    "        # 提取偏置项（最后一个权重）\n",
    "        b_true=torch.tensor(w[-1]).float()\n",
    "        \n",
    "        # 计算真实标签（带多项式）\n",
    "        if num_inputs==1:  # 单特征情况\n",
    "            labels_true=torch.pow(features_ture,deg)*w_true+b_true  # y = x^deg * w + b\n",
    "        else:  # 多特征情况\n",
    "            labels_true=torch.mm(torch.pow(features_ture,deg),w_true)+b_true  # y = X^deg @ w + b\n",
    "        \n",
    "        # 在特征矩阵右侧添加一列全1（用于偏置项）\n",
    "        features=torch.cat((features_ture,torch.ones(size=[len(features_ture),1])),dim=1)\n",
    "        # 在真实标签上添加高斯噪声\n",
    "        labels=labels_true+torch.randn(size=labels_true.shape)*delta\n",
    "    \n",
    "    # 当不包含偏置项时\n",
    "    else:\n",
    "        # 输入特征数量 = 权重数量（所有权重都用于特征）\n",
    "        num_inputs=len(w)\n",
    "        # 生成随机特征矩阵（标准正态分布）\n",
    "        features_ture=torch.randn(num_examples,num_inputs)\n",
    "        # 所有权重都转为列向量\n",
    "        w_true=torch.tensor(w).reshape(-1,1).float()\n",
    "        \n",
    "        # 计算真实标签（带多项式）\n",
    "        if num_inputs==1:  # 单特征情况\n",
    "            labels_true=torch.pow(features_ture,deg)*w_true  # y = x^deg * w\n",
    "        else:  # 多特征情况\n",
    "            labels_true=torch.mm(torch.pow(features_ture,deg),w_true )# y = X^deg @ w\n",
    "        \n",
    "        # 特征矩阵保持不变（不添加偏置列）\n",
    "        features=features_ture\n",
    "        # 在真实标签上添加高斯噪声\n",
    "        labels=labels_true+torch.randn(size=labels_true.shape)*delta\n",
    "    \n",
    "    return features, labels \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc0ec964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000126\n",
      "epoch 2, loss 0.000101\n",
      "epoch 3, loss 0.000102\n"
     ]
    }
   ],
   "source": [
    "# 0.确定数据集\n",
    "features, labels =tensorGenReg()\n",
    "\n",
    "# 1. 选择模型\n",
    "def linreg(X, w):\n",
    "    return torch.mm(X, w)  # 线性回归模型：X × w\n",
    "\n",
    "# 2. 目标函数-损失函数\n",
    "def squared_loss(y_hat, y):\n",
    "    num_ = y.numel()  # 样本数量\n",
    "    sse = torch.sum((y_hat.reshape(-1, 1) - y.reshape(-1, 1)) ** 2)  # 平方误差和\n",
    "    return sse / num_  # 均方误差\n",
    "\n",
    "# 3. 优化算法\n",
    "def sgd(params, lr):\n",
    "    params.data -= lr * params.grad  # 参数更新\n",
    "    params.grad.zero_()  # 梯度清零\n",
    "\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(420)\n",
    "\n",
    "# 初始化参数\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "w = torch.zeros(3, 1, requires_grad=True)  # 3个权重（2特征+1截距）\n",
    "\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "# 模型训练过程（修正后的）\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        # ✅ 关键的三步训练流程：\n",
    "        l = loss(net(X, w), y)  # 前向传播：计算损失\n",
    "        l.backward()            # 反向传播：计算梯度\n",
    "        sgd(w, lr)              # 参数更新：优化权重\n",
    "    \n",
    "    # 每个epoch结束后评估整体性能\n",
    "    train_l = loss(net(features, w), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31101d6",
   "metadata": {},
   "source": [
    "使用summaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4af16a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='reg_loss')\n",
    "# 初始化核心参数\n",
    "batch_size = 10                                # 每一个小批的数量\n",
    "lr = 0.03                                      # 学习率\n",
    "num_epochs = 3                                 # 训练过程遍历几次数据\n",
    "w = torch.zeros(3, 1, requires_grad = True)    # 随机设置初始权重\n",
    "\n",
    "# 参与训练的模型方程\n",
    "net = linreg                                   # 使用回归方程\n",
    "loss = squared_loss                            # 均方误差的一半作为损失函数\n",
    "\n",
    "# 模型训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w), y)\n",
    "        l.backward()\n",
    "        sgd(w, lr)\n",
    "    train_l = loss(net(features, w), labels)\n",
    "    writer.add_scalar('mul', train_l, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82655b55",
   "metadata": {},
   "source": [
    "快速实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4900f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #1.核心参数\n",
    "batch_size = 10                                # 设置每个训练批次包含10个样本\n",
    "lr = 0.03                                      # 设置学习率为0.03，控制参数更新步长\n",
    "num_epochs = 3                                 # 设置训练过程遍历整个数据集3次\n",
    "\n",
    "#2.数据准备\n",
    "# 设随机数种子\n",
    "torch.manual_seed(420)                         # 设置随机种子为420，确保实验结果可重现\n",
    "\n",
    "# 创建数据集\n",
    "features, labels = tensorGenReg()              # 调用数据生成函数创建特征和标签\n",
    "features = features[:, :-1]                    # 剔除特征矩阵最后一列（全1的截距项）\n",
    "data = TensorDataset(features, labels)         # 将特征和标签封装成PyTorch数据集对象\n",
    "batchData = DataLoader(data, batch_size = batch_size, shuffle = True)  # 创建数据加载器，设置批量大小和打乱顺序\n",
    "\n",
    "#3.定义模型\n",
    "class LR(nn.Module):                           # 定义线性回归类，继承自nn.Module\n",
    "    def __init__(self, in_features=2, out_features=1):  # 初始化方法：定义模型的层结构\n",
    "        super(LR, self).__init__()             # 调用父类初始化方法\n",
    "        self.linear = nn.Linear(in_features, out_features)  # 创建线性层，输入2特征，输出1个值\n",
    "        \n",
    "    def forward(self, x):                      # 定义前向传播方法：输入如何通过各层得到输出\n",
    "        out = self.linear(x)                   # 输入数据通过线性层计算输出\n",
    "        return out                             # 返回预测结果\n",
    "\n",
    "# 实例化模型\n",
    "LR_model = LR()                                # 创建线性回归模型的实例\n",
    "\n",
    "#4.损失函数\n",
    "criterion = nn.MSELoss()                       # 定义均方误差损失函数\n",
    "\n",
    "#5.优化方法\n",
    "optimizer = optim.SGD(LR_model.parameters(), lr = 0.03)  # 创建随机梯度下降优化器，传入模型参数和学习率\n",
    "\n",
    "#6.模型训练\n",
    "def fit(net, criterion, optimizer, batchdata, epochs):  # 定义训练函数，接收模型、损失函数、优化器、数据和训练轮数\n",
    "    for epoch in range(epochs):                # 外层循环：遍历训练轮数\n",
    "        for X, y in batchdata:                 # 内层循环：遍历每个批次的数据\n",
    "            yhat = net.forward(X)              # 前向传播：计算当前批次的预测值\n",
    "            loss = criterion(yhat, y)          # 计算损失：比较预测值和真实值的差异\n",
    "            optimizer.zero_grad()              # 梯度清零：清空上一轮的梯度信息，防止累积\n",
    "            loss.backward()                    # 反向传播：计算损失函数关于模型参数的梯度\n",
    "            optimizer.step()                   # 参数更新：根据梯度方向调整模型参数\n",
    "        writer.add_scalar('loss', loss, global_step=epoch)  # 记录当前epoch的损失值到TensorBoard（这行代码不完整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f2a99ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机数种子\n",
    "torch.manual_seed(420)   \n",
    "\n",
    "fit(net = LR_model, \n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer, \n",
    "    batchdata = batchData, \n",
    "    epochs = num_epochs)\n",
    "LR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef323ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.0006, -1.0000]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.0008], requires_grad=True)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型参数\n",
    "list(LR_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b32243a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算MSE\n",
    "criterion(LR_model(features), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2413f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(LR_model, (features,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jrx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
